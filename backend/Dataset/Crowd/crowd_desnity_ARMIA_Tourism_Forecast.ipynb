{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5a3e116",
   "metadata": {},
   "source": [
    "# ARIMA\n",
    "- Produces Crowd Predicitions based on weather and user selected location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd456dd",
   "metadata": {},
   "source": [
    "### Load the Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebb45c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX \n",
    "from pmdarima import auto_arima\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime,timedelta,date\n",
    "import holidays as hl\n",
    "\n",
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "\n",
    "str_trim_date = pd.to_datetime('2025-01-01').to_datetime64()# The start of the dataset\n",
    "end_trim_date = pd.to_datetime('2025-09-30').to_datetime64()# The end of the dataset, shouldn't assume both actual end at this date for each location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e0fcfb",
   "metadata": {},
   "source": [
    "### Load the data set ensure both the dataframe date range and date are in correct format for ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "551ce9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Auck_peds = pd.read_csv(\"data_weather/Final/Auckland_Pedestrian_Hourly.csv\")\n",
    "Dub_peds = pd.read_csv(\"data_weather/Final/Dublin_Pedestrian_Hourly.csv\")\n",
    "\n",
    "df = pd.concat([Auck_peds, Dub_peds],ignore_index=True)\n",
    "\n",
    "df['Date'] = df['Date'].apply(lambda x: pd.to_datetime(x).to_datetime64())\n",
    "df = df.sort_values(['Location_ID','Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "b6258390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['2021-01-01T00:00:00.000000000', '2021-01-02T00:00:00.000000000',\n",
       "        '2021-01-03T00:00:00.000000000', ...,\n",
       "        '2025-09-28T00:00:00.000000000', '2025-09-29T00:00:00.000000000',\n",
       "        '2025-09-30T00:00:00.000000000'], dtype='datetime64[ns]')]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[df['Date'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f310aff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([numpy.datetime64('2025-09-30T00:00:00.000000000')],\n",
       " [numpy.datetime64('2025-01-01T00:00:00.000000000')])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[end_trim_date],[str_trim_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e120d01",
   "metadata": {},
   "source": [
    "### Main ARIMA model\n",
    "- Model creation\n",
    "- Data splitting\n",
    "- Fitting model\n",
    "- Creates pickel files for each location\n",
    "    - Need seperate pickel files for forecasting each location \n",
    "- ARMIA needs to have even spacing between dates\n",
    "    if gap then a fill in needs to be done for Y values(Dep Var) & Exog(or X Ind Vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "ca3ba8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRDUB_1\n",
      "[(249,), (249, 5)]\n",
      "[(174,), (75,), (174, 5), (75, 5)]\n",
      "IRDUB_3\n",
      "[(249,), (249, 5)]\n",
      "[(174,), (75,), (174, 5), (75, 5)]\n",
      "IRDUB_4\n",
      "[(249,), (249, 5)]\n",
      "[(174,), (75,), (174, 5), (75, 5)]\n",
      "IRDUB_5\n",
      "[(249,), (249, 5)]\n",
      "[(174,), (75,), (174, 5), (75, 5)]\n",
      "NZAUK_1\n",
      "[(273,), (273, 5)]\n",
      "[(191,), (82,), (191, 5), (82, 5)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\athar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NZAUK_2\n",
      "[(273,), (273, 5)]\n",
      "[(191,), (82,), (191, 5), (82, 5)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\athar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NZAUK_4\n",
      "[(273,), (273, 5)]\n",
      "[(191,), (82,), (191, 5), (82, 5)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\athar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NZAUK_5\n",
      "[(273,), (273, 5)]\n",
      "[(191,), (82,), (191, 5), (82, 5)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\athar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "for loc in df['Location_ID'].unique():\n",
    "    print(loc)\n",
    "    sub = df[df['Location_ID'] == loc].set_index('Date') # Important for the ARIMA model to function \n",
    "    \n",
    "    y = sub['Avg_Daily_Pedestrian_Count'].asfreq('D').interpolate(method='linear') # D is daily, rate of change fill in\n",
    "    x = sub[['Holiday',\n",
    "                'Weather_Temperature_Avg',\n",
    "                'Weather_Wind_Speed_Avg',\n",
    "                'Weather_Precipitation_Sum',\n",
    "                'Weather_Relative_Humidity_Avg']].asfreq('D').interpolate(method='linear') # numeric only\n",
    "    \n",
    "    y = y[(y.index <= end_trim_date) & (y.index >= str_trim_date)] # will change depending on new datasets in the future\n",
    "    x = x[(x.index <= end_trim_date) & (x.index >= str_trim_date)] # will change depending on new datasets in the future \n",
    "    print([y.shape,x.shape])\n",
    "\n",
    "    split = int(len(y) * 0.7) # keep chrno order \n",
    "\n",
    "    y_train = y.iloc[:split]\n",
    "    y_test  = y.iloc[split:]\n",
    "\n",
    "    # sclar = MinMaxScaler()\n",
    "\n",
    "    # x_train = sclar.fit_transform(x.iloc[:split])\n",
    "    # x_test = sclar.transform(x.iloc[split:])\n",
    "\n",
    "    x_train = x.iloc[:split]\n",
    "    x_test  = x.iloc[split:]\n",
    "\n",
    "    print([y_train.shape,y_test.shape,x_train.shape,x_test.shape])\n",
    "\n",
    "    # Auto-tune ARIMA parameters\n",
    "    stepwise = auto_arima(y=y_train, # Dep\n",
    "                          X=x_train, # Indep\n",
    "                          seasonal=True,\n",
    "                          m=7, # weekly pattern\n",
    "                          trace=False,\n",
    "                          error_action='ignore',\n",
    "                          suppress_warnings=True)\n",
    "    \n",
    "    # Fit SARIMA model\n",
    "    model = SARIMAX(endog=y_train, # Dep\n",
    "                    exog=x_train, # Indep\n",
    "                    order=stepwise.order,\n",
    "                    seasonal_order=stepwise.seasonal_order,\n",
    "                    enforce_stationarity=False, # Variance and trends aren't constant set to false\n",
    "                    enforce_invertibility=False)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "    day = 70\n",
    "\n",
    "    y_pred = results.predict(\n",
    "        start=y_test[:day].index[0],\n",
    "        end=y_test[:day].index[-1],\n",
    "        exog=x_test[:day]\n",
    "    )\n",
    "    \n",
    "    models[f'{loc}'] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc529969",
   "metadata": {},
   "source": [
    "### Save model pickel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "ba01ca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"arima_models\", exist_ok=True) \n",
    "for loc,results in models.items():\n",
    "    # Save model\n",
    "    model_path = f\"arima_models/{loc}_arima.pkl\"\n",
    "    with open(model_path, \"wb\") as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9395d4",
   "metadata": {},
   "source": [
    "#### Use a test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "855471b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Open-Meteo API client with cache and retry on error # <--- this is from Open Meteo Api Docs\n",
    "cache_session = requests_cache.CachedSession('.amriacache', expire_after = 3600)\n",
    "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "openmeteo = openmeteo_requests.Client(session = retry_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "07c57526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Weather_Requester(lat:float,long:float) -> pd.DataFrame:\n",
    "    url = \"https://archive-api.open-meteo.com/v1/archive\" # Histroical Data\n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": long,\n",
    "        \"start_date\": '2025-09-30',\n",
    "        \"end_date\": (date.today()-timedelta(days=1)).strftime('%Y-%m-%d'),\n",
    "        \"daily\": [\"precipitation_sum\", \"temperature_2m_mean\", \"relative_humidity_2m_mean\", \"wind_gusts_10m_mean\"],\n",
    "        \"timezone\": \"America/New_York\",\n",
    "    }\n",
    "    responses = openmeteo.weather_api(url, params=params)\n",
    "    # Basically getting the data for the beginning of the trim point of Sep 30 2025 of the dataset to 1 day - current day   \n",
    "    dly = responses[0].Daily()\n",
    "\n",
    "    P1 = dly.Variables(0).ValuesAsNumpy() # Np array's \n",
    "    T1 = dly.Variables(1).ValuesAsNumpy()\n",
    "    R1 = dly.Variables(2).ValuesAsNumpy()\n",
    "    W1 = dly.Variables(3).ValuesAsNumpy()\n",
    "\n",
    "    url = \"https://seasonal-api.open-meteo.com/v1/seasonal\" # Future Data\n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": long,\n",
    "        \"forecast_days\": 180,\n",
    "        \"timezone\": \"America/New_York\",\n",
    "        \"daily\": [\"temperature_2m_mean\", \"wind_speed_10m_mean\", \"precipitation_sum\", \"relative_humidity_2m_mean\"]\n",
    "    }\n",
    "    responses = openmeteo.weather_api(url, params=params)\n",
    "    dly = responses[0].Daily()\n",
    "\n",
    "    T2 = dly.Variables(0).ValuesAsNumpy()\n",
    "    W2 = dly.Variables(1).ValuesAsNumpy()\n",
    "    P2 = dly.Variables(2).ValuesAsNumpy()\n",
    "    R2 = dly.Variables(3).ValuesAsNumpy()\n",
    "\n",
    "    T = np.concatenate((T1,T2))\n",
    "    w = np.concatenate((W1,W2))\n",
    "    P = np.concatenate((P1,P2))\n",
    "    R = np.concatenate((R1,R2))\n",
    "    \n",
    "    # Build the final indep array, holiday and time will be added later\n",
    "    vstk = pd.DataFrame(data = np.vstack((T,w,P,R)).T,\n",
    "                        columns=['Weather_Temperature_Avg',\n",
    "                                 'Weather_Wind_Speed_Avg',\n",
    "                                 'Weather_Precipitation_Sum',\n",
    "                                 'Weather_Relative_Humidity_Avg'])\n",
    "\n",
    "    return vstk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2556a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Holidayer(df:pd.DataFrame,CCode:str) -> pd.DataFrame:\n",
    "    # Uses country code and each data to find holiday or not\n",
    "    df['Holiday'] = df['Date'].apply(lambda x: 1 if hl.country_holidays(country=CCode).get(x) != None else 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "70ba835d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            predicted_mean\n",
      "2025-07-11     6313.527974\n",
      "2025-07-12     5631.937142\n",
      "2025-07-13     3952.415313\n",
      "2025-07-14     5321.695013\n",
      "2025-07-15     5661.107422\n",
      "...                    ...\n",
      "2026-03-14     9027.422004\n",
      "2026-03-15     6855.277517\n",
      "2026-03-16     8970.497962\n",
      "2026-03-17     9476.668836\n",
      "2026-03-18     9784.316034\n",
      "\n",
      "[251 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "loc = \"NZAUK_1\" # This was a location to be displayed to user\n",
    "with open(f\"arima_models/{loc}_arima.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f) # grab right pickel file\n",
    "\n",
    "d = datetime(2025,11,29).date()# User specfiies a date -- test\n",
    "w = Weather_Requester(-36.8485,174.7633) # Grab weather from past and for future\n",
    "w.insert(0,'Holiday',0)# Inserting these columns to match indep input\n",
    "w.insert(0,'Date',range(len(w))) # Use range to fill in date indexing numbers \n",
    "# Add in the date range from trim point 2025-09-30\n",
    "w['Date'] = w['Date'].apply(lambda x: datetime(2025,9,30).date() + timedelta(days=x))\n",
    "h = Holidayer(w,'IE') # Add in the holiday data\n",
    "h = h.set_index('Date').asfreq('D').interpolate(method='linear') # numeric only\n",
    "# Send to predict next set of days\n",
    "pred_mean = pd.DataFrame(model.get_forecast(exog=h,steps=len(h)).predicted_mean)\n",
    "print(pred_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "80c2abbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2025-07-11 00:00:00')"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_meancp = pred_mean.copy()\n",
    "idx = pred_meancp.index[:1][0] # deaaling with timestamps\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "7a0366f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2025-11-29 00:00:00')"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = pd.Timestamp(d) # convert user specified date\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "952e36a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8784.321120894465]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to get the forecasted crowd number at a date for a lcoation\n",
    "[pred_meancp['predicted_mean'].loc[f]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "c1870f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Timestamp('2025-10-30 00:00:00'), 8673.199239607291)\n",
      "(Timestamp('2025-10-31 00:00:00'), 9248.587278072411)\n",
      "(Timestamp('2025-11-01 00:00:00'), 8034.03073101135)\n",
      "(Timestamp('2025-11-02 00:00:00'), 6457.493610934678)\n",
      "(Timestamp('2025-11-03 00:00:00'), 8026.887653059958)\n",
      "(Timestamp('2025-11-04 00:00:00'), 8478.44526924403)\n",
      "(Timestamp('2025-11-05 00:00:00'), 8805.872951263025)\n",
      "(Timestamp('2025-11-06 00:00:00'), 8476.157840438196)\n",
      "(Timestamp('2025-11-07 00:00:00'), 8853.743776221445)\n",
      "(Timestamp('2025-11-08 00:00:00'), 8081.045105048558)\n",
      "(Timestamp('2025-11-09 00:00:00'), 6404.394135615088)\n",
      "(Timestamp('2025-11-10 00:00:00'), 8244.583391882095)\n",
      "(Timestamp('2025-11-11 00:00:00'), 8684.17733286873)\n",
      "(Timestamp('2025-11-12 00:00:00'), 9069.288528674771)\n",
      "(Timestamp('2025-11-13 00:00:00'), 7123.70280773749)\n",
      "(Timestamp('2025-11-14 00:00:00'), 8931.54895080841)\n",
      "(Timestamp('2025-11-15 00:00:00'), 8230.966023380839)\n",
      "(Timestamp('2025-11-16 00:00:00'), 6624.710422696017)\n",
      "(Timestamp('2025-11-17 00:00:00'), 8246.09476627702)\n",
      "(Timestamp('2025-11-18 00:00:00'), 8788.398437320306)\n",
      "(Timestamp('2025-11-19 00:00:00'), 8913.049287152593)\n",
      "(Timestamp('2025-11-20 00:00:00'), 8620.019551116264)\n",
      "(Timestamp('2025-11-21 00:00:00'), 9133.125541132034)\n",
      "(Timestamp('2025-11-22 00:00:00'), 8306.95492814989)\n",
      "(Timestamp('2025-11-23 00:00:00'), 6676.747894060627)\n",
      "(Timestamp('2025-11-24 00:00:00'), 8362.869468897085)\n",
      "(Timestamp('2025-11-25 00:00:00'), 8924.840616257468)\n",
      "(Timestamp('2025-11-26 00:00:00'), 9323.367182792492)\n",
      "(Timestamp('2025-11-27 00:00:00'), 9103.147885620243)\n",
      "(Timestamp('2025-11-28 00:00:00'), 9381.254291327246)\n",
      "(Timestamp('2025-11-29 00:00:00'), 8784.321120894465)\n",
      "(Timestamp('2025-11-30 00:00:00'), 7013.498993433363)\n",
      "(Timestamp('2025-12-01 00:00:00'), 8922.380692774708)\n",
      "(Timestamp('2025-12-02 00:00:00'), 8830.58105016574)\n",
      "(Timestamp('2025-12-03 00:00:00'), 8869.585985271793)\n",
      "(Timestamp('2025-12-04 00:00:00'), 8727.74043650147)\n",
      "(Timestamp('2025-12-05 00:00:00'), 9246.308312391968)\n",
      "(Timestamp('2025-12-06 00:00:00'), 8534.496604444947)\n",
      "(Timestamp('2025-12-07 00:00:00'), 6777.1529287959875)\n",
      "(Timestamp('2025-12-08 00:00:00'), 8797.74782692913)\n",
      "(Timestamp('2025-12-09 00:00:00'), 9110.491523090801)\n"
     ]
    }
   ],
   "source": [
    "print(*pred_meancp['predicted_mean'].loc[f - timedelta(days=30):f + timedelta(days=10)].items(),sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
